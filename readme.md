# MACHINE LEARNING ACCELERATION

This repository contains experiments, models, and analysis related to **machine learning acceleration techniques**. The focus is on understanding how hardware-aware optimization, inference profiling, and model analysis can improve execution speed, power efficiency, and deployment viability of deep learning workloads.

## ğŸ“‚ Repository Structure

.
â”œâ”€â”€ Lab1/ # Introduction to inference benchmarking
â”œâ”€â”€ Lab2/ # NVIDIA Nsight Compute + resnet18 profiling
â”œâ”€â”€ Lab3/ # Systolic array simulations (e.g., with SCALE-Sim)
â”œâ”€â”€ reports/ # PDF reports, analysis summaries
â”œâ”€â”€ images/ # Supporting graphs and visualizations
â””â”€â”€ README.md # Project overview

yaml
Copy
Edit

---

## ğŸš€ Highlights

- ğŸ“Š **Inference Analysis** using NVIDIA Nsight Compute
- ğŸ§  **CNN Model Profiling** (ResNet18) to understand memory & compute bottlenecks
- ğŸ› ï¸ **Accelerator Simulation** using tools like SCALE-Sim
- ğŸ“ Structured lab submissions and reports for reproducibility

---

## âš™ï¸ Tools & Technologies

- Python (NumPy, Matplotlib)
- Nsight Compute
- TensorFlow / PyTorch (for model exports)
- SCALE-Sim
- Git for version control

---

## ğŸ“ File Sizes and Downloads

> ğŸ”´ Note: Some large binary files (e.g., `.zip`, `.mov`, `.ncu-rep`) have been excluded from this repository due to GitHubâ€™s file size limitations.  
> ğŸ‘‰ These files are available via [Google Drive link or Dropbox] â€” **please refer to the lab report PDFs for download instructions.**

---

## ğŸ§  Learning Outcomes

- Understand latency and throughput trade-offs in ML inference
- Gain hands-on exposure to hardware simulation tools
- Learn how architectural choices affect ML acceleration
- Develop intuition for DRAM bandwidth, array utilization, and on-chip memory tuning

---

## ğŸ“¬ Contact

For any questions or collaborations, feel free to reach out:

**Mainak Malay Saha**  
ğŸ“§ mainaksaha@asu.edu  
ğŸŒ [mainaksaha.in](https://mainaksaha.in)

---

## ğŸ“œ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.